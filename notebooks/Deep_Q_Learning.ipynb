{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j_no2BuvPUE"
      },
      "source": [
        "# Deep Q-Learning \n",
        "\n",
        "Deep Q-Learning uses a neural network to approximate $Q$ functions. Hence, we usually refer to this algorithm as DQN (for *deep Q network*).\n",
        "\n",
        "The parameters of the neural network are denoted by $\\theta$. \n",
        "*   As input, the network takes a state $s$,\n",
        "*   As output, the network returns $Q(s, a, \\theta)$, the value of each action $a$ in state $s$, according to the parameters $\\theta$.\n",
        "\n",
        "\n",
        "The goal of Deep Q-Learning is to learn the parameters $\\theta$ so that $Q(s, a, \\theta)$ approximates well the optimal $Q$-function $Q^*(s, a)$. \n",
        "\n",
        "In addition to the network with parameters $\\theta$, the algorithm keeps another network with the same architecture and parameters $\\theta^-$, called **target network**.\n",
        "\n",
        "The algorithm works as follows:\n",
        "\n",
        "1.   At each time $t$, the agent is in state $s_t$ and has observed the transitions $(s_i, a_i, r_i, s_i')_{i=1}^{t-1}$, which are stored in a **replay buffer**.\n",
        "\n",
        "2.  Choose action $a_t = \\arg\\max_a Q(s_t, a)$ with probability $1-\\varepsilon_t$, and $a_t$=random action with probability $\\varepsilon_t$. \n",
        "\n",
        "3. Take action $a_t$, observe reward $r_t$ and next state $s_t'$.\n",
        "\n",
        "4. Add transition $(s_t, a_t, r_t, s_t')$ to the **replay buffer**.\n",
        "\n",
        "4.  Sample a minibatch $\\mathcal{B}$ containing $B$ transitions from the replay buffer. Using this minibatch, we define the loss:\n",
        "\n",
        "$$\n",
        "L(\\theta) = \\sum_{(s_i, a_i, r_i, s_i') \\in \\mathcal{B}}\n",
        "\\left[\n",
        "Q(s_i, a_i, \\theta) -  y_i\n",
        "\\right]^2\n",
        "$$\n",
        "where the $y_i$ are the **targets** computed with the **target network** $\\theta^-$:\n",
        "\n",
        "$$\n",
        "y_i = r_i + \\gamma \\max_{a'} Q(s_i', a', \\theta^-).\n",
        "$$\n",
        "\n",
        "5. Update the parameters $\\theta$ to minimize the loss, e.g., with gradient descent (**keeping $\\theta^-$ fixed**): \n",
        "$$\n",
        "\\theta \\gets \\theta - \\eta \\nabla_\\theta L(\\theta)\n",
        "$$\n",
        "where $\\eta$ is the optimization learning rate. \n",
        "\n",
        "6. Every $N$ transitions ($t\\mod N$ = 0), update target parameters: $\\theta^- \\gets \\theta$.\n",
        "\n",
        "7. $t \\gets t+1$. Stop if $t = T$, otherwise go to step 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhKHif__t9OD"
      },
      "source": [
        "# Colab setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPM6JvsphJVN",
        "outputId": "0e3e3a69-1567-46c3-80d6-565529b6c73c"
      },
      "outputs": [],
      "source": [
        "%pip install ribs[all] gym~=0.17.0 Box2D~=2.3.10 tqdm\n",
        "import gym\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "env = gym.make(\"LunarLander-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aylqy_sDqebM",
        "outputId": "a9f641f2-22ae-42e9-8e36-f8d6089f4822"
      },
      "outputs": [],
      "source": [
        "# After installing, restart the kernel\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  print(\"Installing packages, please wait a few moments. You may need to restart the runtime after the installation.\")\n",
        "\n",
        "  # install rlberry library\n",
        "  !pip install git+https://github.com/rlberry-py/rlberry.git#egg=rlberry[default] > /dev/null 2>&1\n",
        "\n",
        "  # install gym\n",
        "  !pip install gym[all] > /dev/null 2>&1\n",
        "\n",
        "  # packages required to show video\n",
        "  !pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "  !apt-get update > /dev/null 2>&1\n",
        "  !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWBRfwosfA9f"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from gym.wrappers import Monitor\n",
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35Zzr-xCya5y"
      },
      "outputs": [],
      "source": [
        "# Create directory for saving videos\n",
        "!mkdir videos > /dev/null 2>&1\n",
        "\n",
        "# Initialize display and import function to show videos\n",
        "import rlberry.colab_utils.display_setup\n",
        "from rlberry.colab_utils.display_setup import show_video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLLwJLQlrTxo"
      },
      "outputs": [],
      "source": [
        "# Random number generator\n",
        "import rlberry.seeding as seeding \n",
        "seeder = seeding.Seeder(456)\n",
        "rng = seeder.rng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "528oqsgefIFl"
      },
      "source": [
        "# 1. Define the parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtExtR4dfMbm",
        "outputId": "19a05530-c878-40a5-cc84-0945b4044f48"
      },
      "outputs": [],
      "source": [
        "# Environment\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "\n",
        "# Discount factor\n",
        "GAMMA = 0.999\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "# Capacity of the replay buffer\n",
        "BUFFER_CAPACITY = 10000\n",
        "# Update target net every ... episodes\n",
        "UPDATE_TARGET_EVERY = 30\n",
        "\n",
        "# Initial value of epsilon\n",
        "EPSILON_START = .7\n",
        "# Parameter to decrease epsilon\n",
        "DECREASE_EPSILON = 100\n",
        "# Minimum value of epislon\n",
        "EPSILON_MIN = 0.001\n",
        "\n",
        "# Number of training episodes\n",
        "N_EPISODES = 500\n",
        "\n",
        "# Learning rate\n",
        "LEARNING_RATE = 1e-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g16Je-dhM2Q"
      },
      "source": [
        "# 2. Define the replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jvh82br9hMNt"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = (state, action, reward, next_state, done)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return rng.choice(self.memory, batch_size).tolist()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "# create instance of replay buffer\n",
        "replay_buffer = ReplayBuffer(BUFFER_CAPACITY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCc9WZppi92W"
      },
      "source": [
        "# 3. Define the neural network architecture, objective and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdNz3Jrwi9iS"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic neural net.\n",
        "    \"\"\"\n",
        "    def __init__(self, obs_size, hidden_size, n_actions):\n",
        "        super(Net, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NI9hFJ28jLZ_"
      },
      "outputs": [],
      "source": [
        "# create network and target network\n",
        "hidden_size = 128\n",
        "obs_size = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "q_net = Net(obs_size, hidden_size, n_actions)\n",
        "target_net = Net(obs_size, hidden_size, n_actions)\n",
        "\n",
        "# objective and optimizer\n",
        "objective = nn.MSELoss()\n",
        "optimizer = optim.Adam(params=q_net.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnR8nfoSjZjL"
      },
      "source": [
        "# 4. Implement Deep Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6fT8cKdjmTZ"
      },
      "outputs": [],
      "source": [
        "#\n",
        "#  Some useful functions\n",
        "#\n",
        "\n",
        "def get_q(states):\n",
        "    \"\"\"\n",
        "    Compute Q function for a list of states\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        states_v = torch.FloatTensor([states])\n",
        "        output = q_net.forward(states_v).data.numpy()  # shape (1, len(states), n_actions)\n",
        "    return output[0, :, :]  # shape (len(states), n_actions)\n",
        "\n",
        "def eval_dqn(n_sim=5):\n",
        "    \"\"\"   \n",
        "    Monte Carlo evaluation of DQN agent.\n",
        "\n",
        "    Repeat n_sim times:\n",
        "        * Run the DQN policy until the environment reaches a terminal state (= one episode)\n",
        "        * Compute the sum of rewards in this episode\n",
        "        * Store the sum of rewards in the episode_rewards array.\n",
        "    \"\"\"\n",
        "    env_copy = deepcopy(env)\n",
        "    episode_rewards = np.zeros(n_sim)\n",
        "\n",
        "    for ii in range(n_sim):\n",
        "        state = env_copy.reset()\n",
        "        done = False \n",
        "        while not done:\n",
        "            action = choose_action(state, 0.0)\n",
        "            next_state, reward, done, _ = env_copy.step(action)\n",
        "            episode_rewards[ii] += reward\n",
        "            state = next_state\n",
        "    return episode_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMspDNntkIoe"
      },
      "outputs": [],
      "source": [
        "def choose_action(state, epsilon):\n",
        "    \"\"\"\n",
        "    Return action according to an epsilon-greedy exploration policy\n",
        "    \"\"\"\n",
        "    if np.random.uniform() < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        q = get_q([state])[0]\n",
        "        return q.argmax()    \n",
        "    \n",
        "\n",
        "def update(state, action, reward, next_state, done):\n",
        "    # add data to replay buffer\n",
        "    replay_buffer.push(state, action, reward, next_state, done)\n",
        "    \n",
        "    if len(replay_buffer) < BATCH_SIZE:\n",
        "        return np.inf\n",
        "    \n",
        "    # get batch\n",
        "    transitions = replay_buffer.sample(BATCH_SIZE)\n",
        "\n",
        "    # process batch of (state, action, reward, next_state)\n",
        "    states = np.array([transitions[ii][0] for  ii in range(BATCH_SIZE)])\n",
        "    actions = np.array([transitions[ii][1] for  ii in range(BATCH_SIZE)])\n",
        "    rewards = np.array([transitions[ii][2] for  ii in range(BATCH_SIZE)])\n",
        "    next_states = np.array([transitions[ii][3] for  ii in range(BATCH_SIZE)])\n",
        "    dones = np.array([transitions[ii][4] for  ii in range(BATCH_SIZE)])\n",
        "\n",
        "    # Convert to torch tensors\n",
        "    states_torch = torch.FloatTensor(states)\n",
        "    actions_torch = torch.LongTensor(actions).view(-1,1)\n",
        "    rewards_torch = torch.FloatTensor(rewards).view(-1, 1)\n",
        "    next_states_torch = torch.FloatTensor(next_states)\n",
        "    dones = torch.FloatTensor(dones)\n",
        "\n",
        "    # Q(s_i, a_i)\n",
        "    values = q_net(states_torch)\n",
        "    values = torch.gather(values, dim=1, index=actions_torch)[:, 0]\n",
        "\n",
        "    # max_a Q(s_{i+1}, a)\n",
        "    values_next_states = target_net(next_states_torch).max(dim=1)[0].detach()\n",
        "    assert values_next_states.shape == values.shape\n",
        "\n",
        "    targets = torch.squeeze(rewards_torch) + GAMMA * (1.0 - dones) * values_next_states\n",
        "    loss = objective(values, targets)\n",
        "     \n",
        "    # Optimize the model \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    return loss.data.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIhpKPhkkU4W",
        "outputId": "87fc900c-18ef-44f6-e89f-56911d5ec8c5"
      },
      "outputs": [],
      "source": [
        "\n",
        "#\n",
        "# Train\n",
        "# \n",
        "\n",
        "EVAL_EVERY = 5\n",
        "REWARD_THRESHOLD = 199\n",
        "\n",
        "def train():\n",
        "    state = env.reset()\n",
        "    epsilon = EPSILON_START\n",
        "    ep = 0\n",
        "    total_time = 0\n",
        "    while ep < N_EPISODES:\n",
        "        action = choose_action(state, epsilon)\n",
        "\n",
        "        # take action and update replay buffer and networks\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        loss = update(state, action, reward, next_state, done)\n",
        "\n",
        "        # update state\n",
        "        state = next_state\n",
        "\n",
        "        # end episode if done\n",
        "        if done:\n",
        "            state = env.reset()\n",
        "            ep   += 1\n",
        "            if ( (ep+1)% EVAL_EVERY == 0):\n",
        "                rewards = eval_dqn()\n",
        "                print(\"episode =\", ep+1, \", reward = \", np.mean(rewards))\n",
        "                if np.mean(rewards) >= REWARD_THRESHOLD:\n",
        "                    break\n",
        "\n",
        "            # update target network\n",
        "            if ep % UPDATE_TARGET_EVERY == 0:\n",
        "                target_net.load_state_dict(q_net.state_dict())\n",
        "            # decrease epsilon\n",
        "            epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN) * \\\n",
        "                            np.exp(-1. * ep / DECREASE_EPSILON )    \n",
        "\n",
        "        total_time += 1\n",
        "\n",
        "# Run the training loop\n",
        "train()\n",
        "\n",
        "# Evaluate the final policy\n",
        "rewards = eval_dqn(20)\n",
        "print(\"\")\n",
        "print(\"mean reward after training = \", np.mean(rewards))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8QZwuvjgrMm"
      },
      "source": [
        "# Visualize the DQN policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGcGwOcEfzPz"
      },
      "outputs": [],
      "source": [
        "def render_env(env):\n",
        "  env = deepcopy(env)\n",
        "  env = Monitor(env, './videos', force=True, video_callable=lambda episode: True)\n",
        "  for episode in range(1):\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    env.render()\n",
        "    while not done:\n",
        "        action = action = choose_action(state, 0.0)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        env.render()\n",
        "    env.close()\n",
        "    show_video()\n",
        "\n",
        "render_env(env)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Tutorial_Deep_Q_Learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
