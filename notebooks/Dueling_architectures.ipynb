{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j_no2BuvPUE"
      },
      "source": [
        "# Tutorial - Deep Q-Learning \n",
        "\n",
        "Deep Q-Learning uses a neural network to approximate $Q$ functions. Hence, we usually refer to this algorithm as DQN (for *deep Q network*).\n",
        "\n",
        "The parameters of the neural network are denoted by $\\theta$. \n",
        "*   As input, the network takes a state $s$,\n",
        "*   As output, the network returns $Q(s, a, \\theta)$, the value of each action $a$ in state $s$, according to the parameters $\\theta$.\n",
        "\n",
        "\n",
        "The goal of Deep Q-Learning is to learn the parameters $\\theta$ so that $Q(s, a, \\theta)$ approximates well the optimal $Q$-function $Q^*(s, a)$. \n",
        "\n",
        "In addition to the network with parameters $\\theta$, the algorithm keeps another network with the same architecture and parameters $\\theta^-$, called **target network**.\n",
        "\n",
        "The algorithm works as follows:\n",
        "\n",
        "1.   At each time $t$, the agent is in state $s_t$ and has observed the transitions $(s_i, a_i, r_i, s_i')_{i=1}^{t-1}$, which are stored in a **replay buffer**.\n",
        "\n",
        "2.  Choose action $a_t = \\arg\\max_a Q(s_t, a)$ with probability $1-\\varepsilon_t$, and $a_t$=random action with probability $\\varepsilon_t$. \n",
        "\n",
        "3. Take action $a_t$, observe reward $r_t$ and next state $s_t'$.\n",
        "\n",
        "4. Add transition $(s_t, a_t, r_t, s_t')$ to the **replay buffer**.\n",
        "\n",
        "4.  Sample a minibatch $\\mathcal{B}$ containing $B$ transitions from the replay buffer. Using this minibatch, we define the loss:\n",
        "\n",
        "$$\n",
        "L(\\theta) = \\sum_{(s_i, a_i, r_i, s_i') \\in \\mathcal{B}}\n",
        "\\left[\n",
        "Q(s_i, a_i, \\theta) -  y_i\n",
        "\\right]^2\n",
        "$$\n",
        "where the $y_i$ are the **targets** computed with the **target network** $\\theta^-$:\n",
        "\n",
        "$$\n",
        "y_i = r_i + \\gamma \\max_{a'} Q(s_i', a', \\theta^-).\n",
        "$$\n",
        "\n",
        "5. Update the parameters $\\theta$ to minimize the loss, e.g., with gradient descent (**keeping $\\theta^-$ fixed**): \n",
        "$$\n",
        "\\theta \\gets \\theta - \\eta \\nabla_\\theta L(\\theta)\n",
        "$$\n",
        "where $\\eta$ is the optimization learning rate. \n",
        "\n",
        "6. Every $N$ transitions ($t\\mod N$ = 0), update target parameters: $\\theta^- \\gets \\theta$.\n",
        "\n",
        "7. $t \\gets t+1$. Stop if $t = T$, otherwise go to step 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhKHif__t9OD"
      },
      "source": [
        "# Colab setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAfw1ryI5COp",
        "outputId": "06de4235-fdc1-403e-b6b4-27faa99539d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ribs[all] in /usr/local/lib/python3.7/dist-packages (0.4.0)\n",
            "Requirement already satisfied: gym~=0.17.0 in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: Box2D~=2.3.10 in /usr/local/lib/python3.7/dist-packages (2.3.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym~=0.17.0) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym~=0.17.0) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym~=0.17.0) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym~=0.17.0) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym~=0.17.0) (0.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (3.0.0)\n",
            "Requirement already satisfied: decorator>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (4.4.2)\n",
            "Requirement already satisfied: toml>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (0.10.2)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (2.4.0)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (1.0.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (0.51.2)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ribs[all]) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ribs[all]) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ribs[all]) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ribs[all]) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.51.0->ribs[all]) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.51.0->ribs[all]) (0.34.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.0->ribs[all]) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.0->ribs[all]) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ribs[all]) (1.1.0)\n",
            "INFO: Making new env: LunarLander-v2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%pip install ribs[all] gym~=0.17.0 Box2D~=2.3.10 tqdm\n",
        "import gym\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "env = gym.make(\"LunarLander-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aylqy_sDqebM",
        "outputId": "877d9aed-9b98-4bb7-8c96-478e12c929c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing packages, please wait a few moments. You may need to restart the runtime after the installation.\n"
          ]
        }
      ],
      "source": [
        "# After installing, restart the kernel\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  print(\"Installing packages, please wait a few moments. You may need to restart the runtime after the installation.\")\n",
        "\n",
        "  # install rlberry library\n",
        "  !pip install git+https://github.com/rlberry-py/rlberry.git#egg=rlberry[default] > /dev/null 2>&1\n",
        "\n",
        "  # install gym\n",
        "  !pip install gym[all] > /dev/null 2>&1\n",
        "\n",
        "  # packages required to show video\n",
        "  !pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "  !apt-get update > /dev/null 2>&1\n",
        "  !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWBRfwosfA9f"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from gym.wrappers import Monitor\n",
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msNV7rtZwnZl"
      },
      "outputs": [],
      "source": [
        "# Create directory for saving videos\n",
        "!mkdir videos > /dev/null 2>&1\n",
        "\n",
        "# Initialize display and import function to show videos\n",
        "import rlberry.colab_utils.display_setup\n",
        "from rlberry.colab_utils.display_setup import show_video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDcNzX0nr_qH"
      },
      "outputs": [],
      "source": [
        "# Random number generator\n",
        "import rlberry.seeding as seeding \n",
        "seeder = seeding.Seeder(789)\n",
        "rng = seeder.rng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "528oqsgefIFl"
      },
      "source": [
        "# 1. Define the parameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Discount factor\n",
        "GAMMA = 0.999\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "# Capacity of the replay buffer\n",
        "BUFFER_CAPACITY = 10000\n",
        "# Update target net every ... episodes\n",
        "UPDATE_TARGET_EVERY = 30\n",
        "\n",
        "# Initial value of epsilon\n",
        "EPSILON_START = .7\n",
        "# Parameter to decrease epsilon\n",
        "DECREASE_EPSILON = 100\n",
        "# Minimum value of epislon\n",
        "EPSILON_MIN = 0.001\n",
        "\n",
        "# Number of training episodes\n",
        "N_EPISODES = 500\n",
        "\n",
        "# Learning rate\n",
        "LEARNING_RATE = 1e-3"
      ],
      "metadata": {
        "id": "8lSqAXG81G1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g16Je-dhM2Q"
      },
      "source": [
        "# 2. Define the replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jvh82br9hMNt"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = (state, action, reward, next_state, done)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return rng.choice(self.memory, batch_size).tolist()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "# create instance of replay buffer\n",
        "replay_buffer = ReplayBuffer(BUFFER_CAPACITY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCc9WZppi92W"
      },
      "source": [
        "# 3. Define the neural network architecture, objective and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdNz3Jrwi9iS"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    \"\"\"Neural netword with a dueling architecture\"\"\"\n",
        "    def __init__(self, in_size, h_size, n_actions):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.main = nn.Sequential(nn.Linear(in_size, 2*h_size), nn.ReLU(),\n",
        "                                  #nn.Linear(h_size, 2*h_size), nn.ReLU()\n",
        "                                  )\n",
        "\n",
        "        # block in charge of state value estimation\n",
        "        self.state_block = nn.Sequential(nn.Linear(2*h_size, h_size), nn.ReLU(),\n",
        "                                         nn.Linear(h_size, 1))\n",
        "\n",
        "        # block in charge of advantage value estimation\n",
        "        self.adv_block = nn.Sequential(nn.Linear(2*h_size, h_size), nn.ReLU(),\n",
        "                                       nn.Linear(h_size, n_actions))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.main(x)\n",
        "\n",
        "        v = self.state_block(x)\n",
        "        a = self.adv_block(x)\n",
        "\n",
        "        # compute q values:\n",
        "\n",
        "        q = v + a - a.mean()\n",
        "        return q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnR8nfoSjZjL"
      },
      "source": [
        "# 4. Implement Deep Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6fT8cKdjmTZ"
      },
      "outputs": [],
      "source": [
        "#\n",
        "#  Some useful functions\n",
        "#\n",
        "\n",
        "def get_q(states):\n",
        "    \"\"\"\n",
        "    Compute Q function for a list of states\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        states_v = torch.FloatTensor([states])\n",
        "        output = q_net.forward(states_v).data.numpy()  # shape (1, len(states), n_actions)\n",
        "    return output[0, :, :]  # shape (len(states), n_actions)\n",
        "\n",
        "def eval_dqn(n_sim=5):\n",
        "    \"\"\"   \n",
        "    Monte Carlo evaluation of DQN agent.\n",
        "\n",
        "    Repeat n_sim times:\n",
        "        * Run the DQN policy until the environment reaches a terminal state (= one episode)\n",
        "        * Compute the sum of rewards in this episode\n",
        "        * Store the sum of rewards in the episode_rewards array.\n",
        "    \"\"\"\n",
        "    env_copy = deepcopy(env)\n",
        "    episode_rewards = np.zeros(n_sim)\n",
        "\n",
        "    for ii in range(n_sim):\n",
        "        state = env_copy.reset()\n",
        "        done = False \n",
        "        while not done:\n",
        "            action = choose_action(state, 0.0)\n",
        "            next_state, reward, done, _ = env_copy.step(action)\n",
        "            episode_rewards[ii] += reward\n",
        "            state = next_state\n",
        "    return episode_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMspDNntkIoe"
      },
      "outputs": [],
      "source": [
        "def choose_action(state, epsilon):\n",
        "    \"\"\"\n",
        "    Return action according to an epsilon-greedy exploration policy\n",
        "    \"\"\"\n",
        "    if np.random.uniform() < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        q = get_q([state])[0]\n",
        "        return q.argmax()    \n",
        "\n",
        "def update(state, action, reward, next_state, done):\n",
        "    # add data to replay buffer\n",
        "    replay_buffer.push(state, action, reward, next_state, done)\n",
        "    \n",
        "    if len(replay_buffer) < BATCH_SIZE:\n",
        "        return np.inf\n",
        "    \n",
        "    # get batch\n",
        "    transitions = replay_buffer.sample(BATCH_SIZE)\n",
        "\n",
        "    # process batch of (state, action, reward, next_state)\n",
        "    states = np.array([transitions[ii][0] for  ii in range(BATCH_SIZE)])\n",
        "    actions = np.array([transitions[ii][1] for  ii in range(BATCH_SIZE)])\n",
        "    rewards = np.array([transitions[ii][2] for  ii in range(BATCH_SIZE)])\n",
        "    next_states = np.array([transitions[ii][3] for  ii in range(BATCH_SIZE)])\n",
        "    dones = np.array([transitions[ii][4] for  ii in range(BATCH_SIZE)])\n",
        "\n",
        "    # Convert to torch tensors\n",
        "    states_torch = torch.FloatTensor(states)\n",
        "    actions_torch = torch.LongTensor(actions).view(-1,1)\n",
        "    rewards_torch = torch.FloatTensor(rewards).view(-1, 1)\n",
        "    next_states_torch = torch.FloatTensor(next_states)\n",
        "    dones = torch.FloatTensor(dones)\n",
        "\n",
        "    # Q(s_i, a_i)\n",
        "    values = q_net(states_torch)\n",
        "    values = torch.gather(values, dim=1, index=actions_torch)[:, 0]\n",
        "\n",
        "    # max_a Q(s_{i+1}, a)\n",
        "    values_next_states = target_net(next_states_torch).max(dim=1)[0].detach()\n",
        "    assert values_next_states.shape == values.shape\n",
        "\n",
        "    targets = torch.squeeze(rewards_torch) + GAMMA * (1.0 - dones) * values_next_states\n",
        "    loss = objective(values, targets)\n",
        "     \n",
        "    # Optimize the model \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    return loss.data.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIhpKPhkkU4W"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "\n",
        "EVAL_EVERY = 5\n",
        "REWARD_THRESHOLD = 130\n",
        "\n",
        "# create network and target network\n",
        "hidden_size = 25\n",
        "obs_size = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "EVAL_EVERY = 5\n",
        "REWARD_THRESHOLD = 100\n",
        "\n",
        "# create network and target network\n",
        "hidden_size = 25\n",
        "obs_size = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "print(env.observation_space.shape)\n",
        "\n",
        "q_net = Net(obs_size, hidden_size, n_actions)\n",
        "target_net = Net(obs_size, hidden_size, n_actions)\n",
        "\n",
        "# objective and optimizer\n",
        "objective = nn.MSELoss()\n",
        "optimizer = optim.Adam(params=q_net.parameters(), lr=LEARNING_RATE)\n",
        "loss_l = []\n",
        "\n",
        "def train():\n",
        "    state = env.reset()\n",
        "    epsilon = EPSILON_START\n",
        "    ep = 0\n",
        "    total_time = 0\n",
        "    while ep < N_EPISODES:\n",
        "        action = choose_action(state, epsilon)\n",
        "\n",
        "        # take action and update replay buffer and networks\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        loss = update(state, action, reward, next_state, done)\n",
        "        loss_l.append(loss)\n",
        "        # update state\n",
        "        state = next_state\n",
        "\n",
        "        # end episode if done\n",
        "        if done:\n",
        "            state = env.reset()\n",
        "            ep   += 1\n",
        "            if ( (ep+1)% EVAL_EVERY == 0):\n",
        "                rewards = eval_dqn()\n",
        "                print(\"episode =\", ep+1, \",   reward = \", np.mean(rewards), \",    epsilon\", 100*epsilon)\n",
        "                if np.mean(rewards) >= REWARD_THRESHOLD:\n",
        "                    break\n",
        "\n",
        "            # update target network\n",
        "            if ep % UPDATE_TARGET_EVERY == 0:\n",
        "                target_net.load_state_dict(q_net.state_dict())\n",
        "            # decrease epsilon\n",
        "            epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN) * \\\n",
        "                            np.exp(-1. * ep / DECREASE_EPSILON )    \n",
        "\n",
        "        total_time += 1\n",
        "    plt.plot(np.arange(len(loss_l)),loss_l)\n",
        "    plt.xlabel('Nombre d\\'action prises')\n",
        "    plt.title(\"Evolution de la loss\")\n",
        "    return loss_l\n",
        "# Run the training loop\n",
        "loss_l = train()\n",
        "\n",
        "# Evaluate the final policy\n",
        "rewards = eval_dqn(20)\n",
        "print(\"\")\n",
        "print(\"mean reward after training = \", np.mean(rewards))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_l"
      ],
      "metadata": {
        "id": "mx7dEl02zeZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8QZwuvjgrMm"
      },
      "source": [
        "# Visualize the DQN policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGcGwOcEfzPz"
      },
      "outputs": [],
      "source": [
        "def render_env(env):\n",
        "  env = deepcopy(env)\n",
        "  env = Monitor(env, './videos', force=True, video_callable=lambda episode: True)\n",
        "  for episode in range(1):\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    env.render()\n",
        "    while not done:\n",
        "        action = choose_action(state, 0.0)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        env.render()\n",
        "    env.close()\n",
        "    show_video()\n",
        "\n",
        "render_env(env)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "BEST Dueling architectures",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}